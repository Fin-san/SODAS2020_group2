{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "import tqdm.notebook as tqdm\n",
    "from collections import Counter\n",
    "import lemmy\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "#Text Analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import da_core_news_md as da\n",
    "import lemmy.pipe\n",
    "\n",
    "try:\n",
    "    nlp = da.load()\n",
    "    pipe = lemmy.pipe.load('da')\n",
    "    nlp.add_pipe(pipe, after='tagger')\n",
    "except ValueError:\n",
    "    None\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def houses_boliga(number_houses):\n",
    "    \"\"\"\n",
    "    Returns a list of all ids for houses on boliga\n",
    "    \n",
    "    \"\"\"\n",
    "    house_id = list()\n",
    "    url = \"https://www.boliga.dk/resultat\"\n",
    "    \n",
    "    for i in range(int(number_houses/50)):\n",
    "        new_url = url + f\"?page={i}\"\n",
    "        response = requests.get(new_url)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        ids = soup.find_all(\"a\",{\"class\":\"house-list-item\"})\n",
    "        link_houses = list()\n",
    "\n",
    "        for link in ids:\n",
    "            link_houses.append(re.findall(\"(/\\d{4,}/)\",link[\"href\"])[0].replace(\"/\",\"\"))\n",
    "        \n",
    "        house_id.extend(link_houses)\n",
    "        \n",
    "    return house_id\n",
    "\n",
    "def get_info(id_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take an list with ids of houses on boliga and gets specific data about these ids\n",
    "    \n",
    "    \"\"\"\n",
    "    all_df = list()\n",
    "    new_keys = [\"registeredArea\",\"downPayment\",\"estateUrl\",\"currentArchiveId\",\"forSaleNowId\",\n",
    "                \"foreclosureId\",\"selfsaleEstateId\",\"cleanStreet\",\"estateId\",\"latitude\",\"longitude\",\n",
    "               \"propertyType\",\"priceChangePercentTotal\",\"energyClass\",\"price\",\"rooms\",\"size\",\"lotSize\",\n",
    "               \"floor\",\"buildYear\",\"city\",\"isActive\",\"municipality\",\"zipCode\",\"street\",\n",
    "                \"squaremeterPrice\",\"daysForSale\",\"createdDate\",\"basementSize\",\"views\"]\n",
    "    \n",
    "    for house_id in id_list:\n",
    "        response = requests.get(f'https://api.boliga.dk/api/v2/estate/{house_id}')\n",
    "        response = response.json()\n",
    "        df_dict = {key: response[key] for key in new_keys}\n",
    "        df = pd.DataFrame(df_dict,index=[0])\n",
    "        all_df.append(df)\n",
    "\n",
    "    df = pd.concat(all_df,axis=0,ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_reviews(df):\n",
    "    bodys = list()\n",
    "    #Finder alle ejendomsmæglere, som har mere end 100 huse til salg\n",
    "    for value in df[\"estateUrl\"].values:\n",
    "        estates.append(value[8:15])\n",
    "    numbers = dict(Counter(estates))\n",
    "    over_100 = dict() \n",
    "    for key, value in numbers.items():\n",
    "        if value > 100:\n",
    "            over_100[key] = value\n",
    "    #Kører igennem alle links og finder tilhørende beskrivelse\n",
    "    for link in tqdm.tqdm(df[\"estateUrl\"].values):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "            \n",
    "            if link[8:15] ==\"home.dk\": #Home\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text\"},\"p\")\n",
    "                bodys.extend([x.p.text.replace(\"\\n\",\"\").strip().lower() for x in ids[0:1] if len(x)>1])\n",
    "            elif link[8:15] ==\"ww.skbo\": #skbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"listing-text\"})\n",
    "                bodys.extend([sk.text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").strip().lower() for sk in ids[0:1] if len(sk)>1])\n",
    "            elif link[8:15] == \"www.nyb\": #Nybolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"foldable-spot__container\"})\n",
    "                bodys.extend([ny.text.replace(\"\\n\",\"\").strip().lower() for ny in ids[0:1] if len(ny)>1])\n",
    "            elif link [8:15] == \"ww.elto\": #Eltoft Nielsen\n",
    "                ids = soup.find_all(\"br\")\n",
    "                bodys.extend([elto.text.replace(\"\\n\",\"\").strip().lower() for elto in ids[0:1] if len(elto)>1])\n",
    "            elif link[8:15] == \"www.cla\": #Claus Borg\n",
    "                ids = soup.find_all(\"div\",{\"id\":\"case_content\"})\n",
    "                bodys.extend([cla.text.replace(\"\\n\",\"\").strip().lower() for cla in ids[0:1] if len(cla)>1])\n",
    "            elif link[8:15] == \"www.lok\": #Lokalbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"css-s7itso eknr0ef1\"})\n",
    "                bodys.extend([car.text.replace(\"\\n\",\"\").strip() for car in ids[0:1] if len(car)>1])\n",
    "            elif link[8:15] == \"www.edc\": #EDC Bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description\"})\n",
    "                bodys.extend([edc.text.replace(\"\\n\",\"\").strip().lower() for edc in ids[0:1] if len(edc)>1])\n",
    "            elif link[8:15] == \"adamsch\": #Adam Schnack\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"listing-text\"})\n",
    "                bodys.extend([adam.text.replace(\"\\n\",\"\").strip().lower() for adam in ids[0:1] if len(adam)>1])\n",
    "            elif link[8:20] == \"www.estate.d\": #Estate\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"property-description\"})\n",
    "                bodys.extend([est.text.replace(\"\\n\",\"\").strip().lower() for est in ids[0:1] if len(est)>1])\n",
    "            elif link[8:15] == \"www.bri\": #Brikk Ejendomme\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"prop-user-content\"})\n",
    "                bodys.extend([bri.text.replace(\"\\n\",\"\").strip().lower() for bri in ids[0:1] if len(bri)>1])\n",
    "            elif link[8:15] == \"www.rea\": #Realmæglerne\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text-full\"})\n",
    "                bodys.extend([rea.text.replace(\"\\n\",\"\").strip().lower() for rea in ids[0:1] if len(rea)>1])\n",
    "            elif link[8:15] == \"danboli\": #Danbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"db-description-block\"})\n",
    "                bodys.extend([dan.text.replace(\"\\n\",\"\").strip().lower() for dan in ids[0:1] if len(dan)>1])\n",
    "            elif link[8:15] == \"ww.lili\": #Lillenhof\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"inner\"})\n",
    "                bodys.extend([dan.text.replace(\"\\n\",\"\").strip().lower() for dan in ids[0:1] if len(dan)>10])\n",
    "            elif link[8:15] == \"bjornby\":\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"content d-md-block d-none wrap-content\"})\n",
    "                bodys.extend([bjor.text.replace(\"\\n\",\"\").strip() for bjor in ids[0:1] if len(bjor)>10])\n",
    "            elif link[8:15] == 'www.hov': #Hovmand\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([hov.text.replace(\"\\n\",\"\").strip() for hov in ids[0:1] if len(hov)>1])\n",
    "            elif link[8:15] == 'ww.jesp': #Jesper Nielsen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"case-description\"})\n",
    "                bodys.extend([jesp.text.replace(\"\\n\",\"\").strip() for jesp in ids[0:1] if len(jesp)>1])\n",
    "            elif link[8:15] == \"www.sel\": #Selvsalg\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"tab-pane active fade in\"})\n",
    "                bodys.extend([selv.text.replace(\"\\n\",\"\").strip() for selv in ids[0:1] if len(selv)>1])\n",
    "            elif link[8:15] == \"www.bol\": #Bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([bol.text.replace(\"\\n\",\"\").strip() for bol in ids[0:1] if len(bol)>1])\n",
    "            elif link[8:15] == 'www.joh': #Johns\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([john.text.replace(\"\\n\",\"\").strip() for john in ids[0:1] if len(john)>1])\n",
    "            elif link[8:15] == \"racking\": #Robinhus\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text-container\"})\n",
    "                bodys.extend([robin.text.replace(\"\\n\",\"\").strip() for robin in ids[0:1] if len(robin)>1])\n",
    "            elif link[8:15] == \"www.min\": #minbolighandel\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([minb.text.replace(\"\\n\",\"\").strip() for minb in ids[0:1] if len(minb)>1])\n",
    "            elif link[8:15] == \"ww.unni\": #Unnibolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([un.text.replace(\"\\n\",\"\").strip() for un in ids[0:1] if len(un)>1])\n",
    "            elif link[8:15] == \"www.sdb\": #Sdb bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([un.text.replace(\"\\n\",\"\").strip() for un in ids[0:1] if len(un)>1])\n",
    "            elif link[8:15] == \"ww.land\":#Landobolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-md-8\"})\n",
    "                bodys.extend([land.text.replace(\"\\n\",\"\").strip() for land in ids[0:1] if len(land)>1])\n",
    "            elif link[8:15] == \"www.ber\": #Bermistof\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([ber.text.replace(\"\\n\",\"\").strip() for ber in ids[0:1] if len(ber)>1])\n",
    "            elif link [8:20] == 'www.carlsber': #Carlsberg Byen\n",
    "                ids = soup.find_all(\"div\",{\"itemprop\":\"description\"})\n",
    "                bodys.extend([car.text.replace(\"\\n\",\"\").strip() for car in ids[0:1] if len(car)>1])\n",
    "            elif link[8:15] == \"www.car\": #Carsten Nordbo\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([car.text.replace(\"\\n\",\"\").strip() for car in ids[0:1] if len(car)>1])\n",
    "            elif link[8:15] == 'ww.agri': \n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-md-8 col-sm-7 hidden-xs text-box desktop\"})\n",
    "                bodys.extend([agr.text.replace(\"\\n\",\"\").strip() for agr in ids[0:1] if len(agr)>1])\n",
    "            elif link[8:15] == \"www.pla\":#Place2Live\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-lg-16\"})\n",
    "                bodys.extend([pla.text.replace(\"\\n\",\"\").strip() for pla in ids[0:1] if len(pla)>1])\n",
    "            elif link[8:15] == \"www.vil\": #Villadsenbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([vil.text.replace(\"\\n\",\"\").strip() for vil in ids[0:1] if len(vil)>1])\n",
    "            elif link[8:15] == 'maegler': #Mæglerhuset\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"case-text\"})\n",
    "                bodys.extend([mae.text.replace(\"\\n\",\"\").strip() for mae in ids[0:1] if len(mae)>1])\n",
    "            elif link[8:15] == 'ww.thom': #ThomasJørgensen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([thom.text.replace(\"\\n\",\"\").strip() for thom in ids[0:1] if len(thom)>1])\n",
    "            elif link[8:15] == 'www.htb': #HTbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"left-side global-style\"})\n",
    "                bodys.extend([htb.text.replace(\"\\n\",\"\").strip() for htb in ids[0:1] if len(htb)>1])\n",
    "            elif link[8:15] == 'ww.boli': #Boligone\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([bol.text.replace(\"\\n\",\"\").strip() for bol in ids[0:1] if len(bol)>1])\n",
    "            elif link[8:15] == \"www.mæg\":#Mæglerringen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([ma.text.replace(\"\\n\",\"\").strip() for ma in ids[0:1] if len(ma)>1])\n",
    "            elif link[8:15] == \"ww.vest\":\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([vest.text.replace(\"\\n\",\"\").strip() for vest in ids[0:1] if len(vest)>1])\n",
    "            elif link[8:15] == \"www.tho\": #Thorregård\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"annonce rammebaggrund\"})\n",
    "                bodys.extend([th.text.replace(\"\\n\",\"\").strip() for th in ids[0:1] if len(th)>1])\n",
    "            elif link[8:15] == \"byggegr\": #Byggegrund\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"section section-12\"})\n",
    "                bodys.extend([byg.text.replace(\"\\n\",\"\").strip() for byg in ids[0:1] if len(byg)>1])\n",
    "            elif link[8:15] == \"grundsa\": #Grundsalg\n",
    "                bodys.append(np.nan)\n",
    "            elif link[8:15] == \"rundsal\": #Grundsalg\n",
    "                bodys.append(np.nan)\n",
    "            elif link[8:15] ==\"ww.paul\": #paulun\n",
    "                bodys.append(np.nan)\n",
    "            else:\n",
    "                bodys.append(np.nan)\n",
    "                if link[8:15] in over_100:\n",
    "                    print(\"Missing\", link[8:15])\n",
    "        except:\n",
    "            print(link,\"virkede ikke\")\n",
    "            continue\n",
    "    \n",
    "    return bodys\n",
    "\n",
    "def find_realtors(df):\n",
    "    \"\"\"\n",
    "    This function finds all realtors, who has more that 100 houses for sale.\n",
    "    Used to find the structure for all realtors of relevance\n",
    "    \"\"\"\n",
    "    realtors_link = list()\n",
    "    #Finder alle ejendomsmæglere, som har mere end 100 huse til salg\n",
    "    estates = list()\n",
    "    for value in df[\"estateUrl\"].values:\n",
    "        estates.append(value[8:15])\n",
    "    numbers = dict(Counter(estates))\n",
    "\n",
    "    over_100 = dict() \n",
    "    for key, value in numbers.items():\n",
    "        if value > 100:\n",
    "            over_100[key] = value\n",
    "    already_accounted = list()\n",
    "    for link in tqdm.tqdm(df[\"estateUrl\"].values):        \n",
    "        if link[8:15] in over_100.keys():\n",
    "            if link[8:15] not in already_accounted:\n",
    "                print(link,\"not in loop\")\n",
    "                print(link[8:15])\n",
    "                realtors_link.append(link[8:15])\n",
    "                already_accounted.append(link[8:15])\n",
    "\n",
    "    return realtors_link\n",
    "    \n",
    "def add_keyattr(df):\n",
    "    key_attr = pd.read_csv(\"keywords.csv\",sep=\";\") #Getting keyword file\n",
    "    #Finding all words corresponding to group\n",
    "    ref_list = [key_attr.loc[(key_attr[\"group\"] == \"view_list\")|(key_attr[\"group_2\"] == \"view_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"view_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"nature_list\")|(key_attr[\"group_2\"] == \"nature_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"nature_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"interior_list\")|(key_attr[\"group_2\"] == \"interior_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"interior_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"location_list\")|(key_attr[\"group_2\"] == \"location_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"location_list\"), \"word\"],\n",
    "               key_attr.loc[(key_attr[\"group\"] == \"other_list\")|(key_attr[\"group_2\"] == \"other_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"other_list\"), \"word\"]]\n",
    "    \n",
    "    #Generation dict for attr\n",
    "    dict_att = {\"view\":list(),\n",
    "                \"nature\":list(),\n",
    "                \"interior\":list(),\n",
    "                \"location\":list(),\n",
    "               \"other\":list()}\n",
    "    \n",
    "    for body in df[\"bodys\"].tolist():\n",
    "        #Generating string\n",
    "        nouns = preprocess_text(body)\n",
    "        for value,cross_list in zip(dict_att.values(),ref_list):\n",
    "            #Generating view key attr\n",
    "            value.append(len(list(set(nouns).intersection(cross_list))))\n",
    "    \n",
    "    added_df = pd.DataFrame(dict_att)\n",
    "    df = df.join(added_df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_csv(csv):\n",
    "    \"\"\"\n",
    "    This function loads the dataset from boliga annd preproccesses it.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "    y = df[\"price\"]\n",
    "    \n",
    "    pipe_preprocess = make_pipeline(StandardScaler())\n",
    "    \n",
    "    #Removing houses not placed in Denmark\n",
    "    df = df[(df[\"latitude\"]<60) & (df[\"latitude\"]>53) & (df[\"longitude\"]>7) & (df[\"longitude\"]<16)]\n",
    "    \n",
    "    #Making room per sqm\n",
    "    df[\"rooms_per_sqm\"] = (df[\"rooms\"]/df[\"size\"]).replace(np.inf,0)\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    prop_temp = df[\"propertyType\"]\n",
    "    kom_temp = df[\"kommune_navn\"]\n",
    "    floor_temp = df[\"floor\"]\n",
    "    \n",
    "    #Dropping all non essential columns\n",
    "    df.drop([\"Unnamed: 0\",\"downPayment\",\"estateUrl\",\"currentArchiveId\",\"forSaleNowId\",\n",
    "            \"foreclosureId\",\"cleanStreet\",\"estateId\",\"latitude\",\"longitude\",\"energyClass\",\n",
    "            \"price\",\"city\",\"isActive\",\"municipality\",\"zipCode\",\"street\",\"createdDate\",\n",
    "            \"squaremeterPrice\",\"region\",\"kommune_nr\",\"rooms\",\"propertyType\",\"kommune_navn\"],axis=1,inplace=True)\n",
    "    df = pd.DataFrame(pipe_preprocess.fit_transform(df))\n",
    "    \n",
    "    #Generating dummy variables - Property type\n",
    "    housing_type={1:'villa',2:'raekkehuse', 3:'ejerlejlighed',4:'fritidshus', 5:'andel',6:'landejendom', \n",
    "              7:'helrsgrund',8:'fritidsgrund', 9:'villalejlighed',10:'andet_1',11:\"andet_2\",12:\"andet_3\"}\n",
    "    \n",
    "    dummy_df = pd.get_dummies(prop_temp.replace(housing_type))   \n",
    "    df = pd.concat([df,dummy_df],axis=1)\n",
    "    \n",
    "    #df.drop(\"propertyType\",axis=1,inplace=True)\n",
    "    #df.rename(columns = housing_type,inplace=True)\n",
    "    \n",
    "    #Dropping all None house\n",
    "    df = df[(df[\"helrsgrund\"] == 0) & (df[\"andet_2\"] == 0) & (df[\"andet_3\"] == 0)]\n",
    "    \n",
    "    #Generation dummies variables - Kommune\n",
    "    dummy_mun = pd.get_dummies(kom_temp)\n",
    "    df = pd.concat([df,dummy_mun],axis=1)\n",
    "    #df.drop(\"kommune_navn\",inplace=True,axis=1)\n",
    "    \n",
    "    #Generation dummies variables - Floor\n",
    "    dummy_floor = pd.get_dummies(floor_temp.map(str))\n",
    "    df = pd.concat([df,dummy_floor],axis=1)\n",
    "    \n",
    "    #df.drop(\"floor\",inplace=True,axis=1)\n",
    "    \n",
    "    #Adding keyattr to df\n",
    "    #df = add_keyattr(df)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    y = y.iloc[df.index].values\n",
    "    X = df.values\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "def preprocess_text(string,nlp=nlp):\n",
    "    \"\"\"\n",
    "    This function takes a string and returns a list with all noun from string lemmatized\n",
    "    \"\"\"\n",
    "\n",
    "    #Removing everything but words\n",
    "    string = re.sub(r'[^\\w\\s]','',string)\n",
    "    \n",
    "    #Removing stopwords\n",
    "    stop_words_list = stopwords.words(\"danish\")\n",
    "    string = [i for i in nltk.word_tokenize(string.lower()) if i not in stop_words_list]\n",
    "    string = \" \".join(string)\n",
    "    \n",
    "    #Getting all nounce\n",
    "    string = nlp(string)\n",
    "    nouns = [word._.lemmas[0] for word in string if word.pos_ == \"NOUN\"]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "def words_count(list_of_strings):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings and returns a dict with counts of each word\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = list_of_strings\n",
    "    counts = dict(Counter(sentences))\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def keywords(body):\n",
    "    \"\"\"\n",
    "    This function takes a list of different strings and returns a dataframe with each word and word count\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = list()\n",
    "    \n",
    "    for bodys in tqdm.tqdm(body):\n",
    "        string = preprocess_text(bodys[0])\n",
    "        word_list.extend(string)\n",
    "        \n",
    "    dict_count = words_count(word_list)\n",
    "    df = pd.DataFrame({\"word\":list(dict_count.keys()),\n",
    "                        \"count\":list(dict_count.values())}).set_index(\"word\")\n",
    "    \n",
    "    #Getting the 300 largest counts returns the keywords used for key attr list\n",
    "    return df\n",
    "    \n",
    "def add_lonlat(df,df_station):\n",
    "    \"\"\"\n",
    "    This function takes two df and returns the distance between two locations of the dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    min_dist = list()\n",
    "    for location in tqdm(df[\"location\"]):\n",
    "        distance = list()\n",
    "        for lon,lat in zip(df_station[\"lon\"],df_station[\"lat\"]):\n",
    "            distance.append(geodesic((lat,lon), location).km)\n",
    "    \n",
    "        min_dist.append(min(distance))\n",
    "        \n",
    "    df[\"dist_station\"] = min_dist\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, df = preprocess_csv(\"house_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
