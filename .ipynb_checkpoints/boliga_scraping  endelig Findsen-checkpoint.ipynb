{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "import tqdm.notebook as tqdm\n",
    "from collections import Counter\n",
    "import lemmy\n",
    "plt.style.use(\"classic\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "#Text Analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import da_core_news_md as da\n",
    "import lemmy.pipe\n",
    "\n",
    "try:\n",
    "    nlp = da.load()\n",
    "    pipe = lemmy.pipe.load('da')\n",
    "    nlp.add_pipe(pipe, after='tagger')\n",
    "except ValueError:\n",
    "    None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def houses_boliga(number_houses):\n",
    "    \"\"\"\n",
    "    Returns a list of all ids for houses on boliga\n",
    "    \n",
    "    \"\"\"\n",
    "    house_id = list()\n",
    "    url = \"https://www.boliga.dk/resultat\"\n",
    "    \n",
    "    for i in range(int(number_houses/50)):\n",
    "        new_url = url + f\"?page={i}\"\n",
    "        response = requests.get(new_url)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        ids = soup.find_all(\"a\",{\"class\":\"house-list-item\"})\n",
    "        link_houses = list()\n",
    "\n",
    "        for link in ids:\n",
    "            link_houses.append(re.findall(\"(/\\d{4,}/)\",link[\"href\"])[0].replace(\"/\",\"\"))\n",
    "        \n",
    "        house_id.extend(link_houses)\n",
    "        \n",
    "    return house_id\n",
    "\n",
    "def get_info(id_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take an list with ids of houses on boliga and gets specific data about these ids\n",
    "    \n",
    "    \"\"\"\n",
    "    all_df = list()\n",
    "    new_keys = [\"registeredArea\",\"downPayment\",\"estateUrl\",\"currentArchiveId\",\"forSaleNowId\",\n",
    "                \"foreclosureId\",\"selfsaleEstateId\",\"cleanStreet\",\"estateId\",\"latitude\",\"longitude\",\n",
    "               \"propertyType\",\"priceChangePercentTotal\",\"energyClass\",\"price\",\"rooms\",\"size\",\"lotSize\",\n",
    "               \"floor\",\"buildYear\",\"city\",\"isActive\",\"municipality\",\"zipCode\",\"street\",\n",
    "                \"squaremeterPrice\",\"daysForSale\",\"createdDate\",\"basementSize\",\"views\"]\n",
    "    \n",
    "    for house_id in id_list:\n",
    "        response = requests.get(f'https://api.boliga.dk/api/v2/estate/{house_id}')\n",
    "        response = response.json()\n",
    "        df_dict = {key: response[key] for key in new_keys}\n",
    "        df = pd.DataFrame(df_dict,index=[0])\n",
    "        all_df.append(df)\n",
    "\n",
    "    df = pd.concat(all_df,axis=0,ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_reviews(df):\n",
    "    bodys = list()\n",
    "    #Finder alle ejendomsmæglere, som har mere end 100 huse til salg\n",
    "    for value in df[\"estateUrl\"].values:\n",
    "        estates.append(value[8:15])\n",
    "    numbers = dict(Counter(estates))\n",
    "    over_100 = dict() \n",
    "    for key, value in numbers.items():\n",
    "        if value > 100:\n",
    "            over_100[key] = value\n",
    "    #Kører igennem alle links og finder tilhørende beskrivelse\n",
    "    for link in tqdm.tqdm(df[\"estateUrl\"].values):\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "            \n",
    "            if link[8:15] ==\"home.dk\": #Home\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text\"},\"p\")\n",
    "                bodys.extend([x.p.text.replace(\"\\n\",\"\").strip().lower() for x in ids[0:1] if len(x)>1])\n",
    "            elif link[8:15] ==\"ww.skbo\": #skbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"listing-text\"})\n",
    "                bodys.extend([sk.text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").strip().lower() for sk in ids[0:1] if len(sk)>1])\n",
    "            elif link[8:15] == \"www.nyb\": #Nybolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"foldable-spot__container\"})\n",
    "                bodys.extend([ny.text.replace(\"\\n\",\"\").strip().lower() for ny in ids[0:1] if len(ny)>1])\n",
    "            elif link [8:15] == \"ww.elto\": #Eltoft Nielsen\n",
    "                ids = soup.find_all(\"br\")\n",
    "                bodys.extend([elto.text.replace(\"\\n\",\"\").strip().lower() for elto in ids[0:1] if len(elto)>1])\n",
    "            elif link[8:15] == \"www.cla\": #Claus Borg\n",
    "                ids = soup.find_all(\"div\",{\"id\":\"case_content\"})\n",
    "                bodys.extend([cla.text.replace(\"\\n\",\"\").strip().lower() for cla in ids[0:1] if len(cla)>1])\n",
    "            elif link[8:15] == \"www.lok\": #Lokalbolig\n",
    "                ids = soup.find_all(\"p\")\n",
    "                loka = [lok.text.replace(\"\\n\",\"\").strip().lower() for lok in ids if len(lok.text)>100]\n",
    "                bodys.extend([''.join(loka)])\n",
    "            elif link[8:15] == \"www.edc\": #EDC Bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description\"})\n",
    "                bodys.extend([edc.text.replace(\"\\n\",\"\").strip().lower() for edc in ids[0:1] if len(edc)>1])\n",
    "            elif link[8:15] == \"adamsch\": #Adam Schnack\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"listing-text\"})\n",
    "                bodys.extend([adam.text.replace(\"\\n\",\"\").strip().lower() for adam in ids[0:1] if len(adam)>1])\n",
    "            elif link[8:20] == \"www.estate.d\": #Estate\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"property-description\"})\n",
    "                bodys.extend([est.text.replace(\"\\n\",\"\").strip().lower() for est in ids[0:1] if len(est)>1])\n",
    "            elif link[8:15] == \"www.bri\": #Brikk Ejendomme\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"prop-user-content\"})\n",
    "                bodys.extend([bri.text.replace(\"\\n\",\"\").strip().lower() for bri in ids[0:1] if len(bri)>1])\n",
    "            elif link[8:15] == \"www.rea\": #Realmæglerne\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text-full\"})\n",
    "                bodys.extend([rea.text.replace(\"\\n\",\"\").strip().lower() for rea in ids[0:1] if len(rea)>1])\n",
    "            elif link[8:15] == \"danboli\": #Danbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"db-description-block\"})\n",
    "                bodys.extend([dan.text.replace(\"\\n\",\"\").strip().lower() for dan in ids[0:1] if len(dan)>1])\n",
    "            elif link[8:15] == \"ww.lili\": #Lillenhof\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"inner\"})\n",
    "                bodys.extend([dan.text.replace(\"\\n\",\"\").strip().lower() for dan in ids[0:1] if len(dan)>10])\n",
    "            elif link[8:15] == \"bjornby\":\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"content d-md-block d-none wrap-content\"})\n",
    "                bodys.extend([bjor.text.replace(\"\\n\",\"\").strip() for bjor in ids[0:1] if len(bjor)>10])\n",
    "            elif link[8:15] == 'www.hov': #Hovmand\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([hov.text.replace(\"\\n\",\"\").strip() for hov in ids[0:1] if len(hov)>1])\n",
    "            elif link[8:15] == 'ww.jesp': #Jesper Nielsen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"case-description\"})\n",
    "                bodys.extend([jesp.text.replace(\"\\n\",\"\").strip() for jesp in ids[0:1] if len(jesp)>1])\n",
    "            elif link[8:15] == \"www.sel\": #Selvsalg\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"tab-pane active fade in\"})\n",
    "                bodys.extend([selv.text.replace(\"\\n\",\"\").strip() for selv in ids[0:1] if len(selv)>1])\n",
    "            elif link[8:15] == \"www.bol\": #Bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([bol.text.replace(\"\\n\",\"\").strip() for bol in ids[0:1] if len(bol)>1])\n",
    "            elif link[8:15] == 'www.joh': #Johns\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([john.text.replace(\"\\n\",\"\").strip() for john in ids[0:1] if len(john)>1])\n",
    "            elif link[8:15] == \"racking\": #Robinhus\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"text-container\"})\n",
    "                bodys.extend([robin.text.replace(\"\\n\",\"\").strip() for robin in ids[0:1] if len(robin)>1])\n",
    "            elif link[8:15] == \"www.min\": #minbolighandel\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([minb.text.replace(\"\\n\",\"\").strip() for minb in ids[0:1] if len(minb)>1])\n",
    "            elif link[8:15] == \"ww.unni\": #Unnibolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([un.text.replace(\"\\n\",\"\").strip() for un in ids[0:1] if len(un)>1])\n",
    "            elif link[8:15] == \"www.sdb\": #Sdb bolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([un.text.replace(\"\\n\",\"\").strip() for un in ids[0:1] if len(un)>1])\n",
    "            elif link[8:15] == \"ww.land\":#Landobolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-md-8\"})\n",
    "                bodys.extend([land.text.replace(\"\\n\",\"\").strip() for land in ids[0:1] if len(land)>1])\n",
    "            elif link[8:15] == \"www.ber\": #Bermistof\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"column\"})\n",
    "                bodys.extend([ber.text.replace(\"\\n\",\"\").strip() for ber in ids[0:1] if len(ber)>1])\n",
    "            elif link [8:20] == 'www.carlsber': #Carlsberg Byen\n",
    "                ids = soup.find_all(\"div\",{\"itemprop\":\"description\"})\n",
    "                bodys.extend([car.text.replace(\"\\n\",\"\").strip() for car in ids[0:1] if len(car)>1])\n",
    "            elif link[8:15] == \"www.car\": #Carsten Nordbo\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([car.text.replace(\"\\n\",\"\").strip() for car in ids[0:1] if len(car)>1])\n",
    "            elif link[8:15] == 'ww.agri': \n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-md-8 col-sm-7 hidden-xs text-box desktop\"})\n",
    "                bodys.extend([agr.text.replace(\"\\n\",\"\").strip() for agr in ids[0:1] if len(agr)>1])\n",
    "            elif link[8:15] == \"www.pla\":#Place2Live\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"col-lg-16\"})\n",
    "                bodys.extend([pla.text.replace(\"\\n\",\"\").strip() for pla in ids[0:1] if len(pla)>1])\n",
    "            elif link[8:15] == \"www.vil\": #Villadsenbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([vil.text.replace(\"\\n\",\"\").strip() for vil in ids[0:1] if len(vil)>1])\n",
    "            elif link[8:15] == 'maegler': #Mæglerhuset\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"case-text\"})\n",
    "                bodys.extend([mae.text.replace(\"\\n\",\"\").strip() for mae in ids[0:1] if len(mae)>1])\n",
    "            elif link[8:15] == 'ww.thom': #ThomasJørgensen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"description col-md-16\"})\n",
    "                bodys.extend([thom.text.replace(\"\\n\",\"\").strip() for thom in ids[0:1] if len(thom)>1])\n",
    "            elif link[8:15] == 'www.htb': #HTbolig\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"left-side global-style\"})\n",
    "                bodys.extend([htb.text.replace(\"\\n\",\"\").strip() for htb in ids[0:1] if len(htb)>1])\n",
    "            elif link[8:15] == 'ww.boli': #Boligone\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([bol.text.replace(\"\\n\",\"\").strip() for bol in ids[0:1] if len(bol)>1])\n",
    "            elif link[8:15] == \"www.mæg\":#Mæglerringen\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([ma.text.replace(\"\\n\",\"\").strip() for ma in ids[0:1] if len(ma)>1])\n",
    "            elif link[8:15] == \"ww.vest\":\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"first-col\"})\n",
    "                bodys.extend([vest.text.replace(\"\\n\",\"\").strip() for vest in ids[0:1] if len(vest)>1])\n",
    "            elif link[8:15] == \"www.tho\": #Thorregård\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"annonce rammebaggrund\"})\n",
    "                bodys.extend([th.text.replace(\"\\n\",\"\").strip() for th in ids[0:1] if len(th)>1])\n",
    "            elif link[8:15] == \"byggegr\": #Byggegrund\n",
    "                ids = soup.find_all(\"div\",{\"class\":\"section section-12\"})\n",
    "                bodys.extend([byg.text.replace(\"\\n\",\"\").strip() for byg in ids[0:1] if len(byg)>1])\n",
    "            elif link[8:15] == \"grundsa\": #Grundsalg\n",
    "                bodys.append(np.nan)\n",
    "            elif link[8:15] == \"rundsal\": #Grundsalg\n",
    "                bodys.append(np.nan)\n",
    "            elif link[8:15] ==\"ww.paul\": #paulun\n",
    "                bodys.append(np.nan)\n",
    "            else:\n",
    "                bodys.append(np.nan)\n",
    "                if link[8:15] in over_100:\n",
    "                    print(\"Missing\", link[8:15])\n",
    "        except:\n",
    "            print(link,\"virkede ikke\")\n",
    "            continue\n",
    "    \n",
    "    return bodys\n",
    "\n",
    "def find_realtors(df):\n",
    "    \"\"\"\n",
    "    This function finds all realtors, who has more that 100 houses for sale.\n",
    "    Used to find the structure for all realtors of relevance\n",
    "    \"\"\"\n",
    "    realtors_link = list()\n",
    "    #Finder alle ejendomsmæglere, som har mere end 100 huse til salg\n",
    "    estates = list()\n",
    "    for value in df[\"estateUrl\"].values:\n",
    "        estates.append(value[8:15])\n",
    "    numbers = dict(Counter(estates))\n",
    "\n",
    "    over_100 = dict() \n",
    "    for key, value in numbers.items():\n",
    "        if value > 100:\n",
    "            over_100[key] = value\n",
    "    already_accounted = list()\n",
    "    for link in tqdm.tqdm(df[\"estateUrl\"].values):        \n",
    "        if link[8:15] in over_100.keys():\n",
    "            if link[8:15] not in already_accounted:\n",
    "                print(link,\"not in loop\")\n",
    "                print(link[8:15])\n",
    "                realtors_link.append(link[8:15])\n",
    "                already_accounted.append(link[8:15])\n",
    "\n",
    "    return realtors_link\n",
    "    \n",
    "def add_keyattr(df):\n",
    "    key_attr = pd.read_csv(\"keywords.csv\",sep=\";\") #Getting keyword file\n",
    "    #Finding all words corresponding to group\n",
    "    ref_list = [key_attr.loc[(key_attr[\"group\"] == \"view_list\")|(key_attr[\"group_2\"] == \"view_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"view_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"nature_list\")|(key_attr[\"group_2\"] == \"nature_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"nature_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"interior_list\")|(key_attr[\"group_2\"] == \"interior_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"interior_list\"), \"word\"],\n",
    "                key_attr.loc[(key_attr[\"group\"] == \"location_list\")|(key_attr[\"group_2\"] == \"location_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"location_list\"), \"word\"],\n",
    "               key_attr.loc[(key_attr[\"group\"] == \"other_list\")|(key_attr[\"group_2\"] == \"other_list\")|\\\n",
    "                         (key_attr[\"group_3\"] == \"other_list\"), \"word\"]]\n",
    "    \n",
    "    #Generation dict for attr\n",
    "    dict_att = {\"view\":list(),\n",
    "                \"nature\":list(),\n",
    "                \"interior\":list(),\n",
    "                \"location\":list(),\n",
    "               \"other\":list()}\n",
    "    \n",
    "    for body in df[\"bodys\"].tolist():\n",
    "        #Generating string\n",
    "        nouns = preprocess_text(body)\n",
    "        for value,cross_list in zip(dict_att.values(),ref_list):\n",
    "            #Generating view key attr\n",
    "            value.append(len(list(set(nouns).intersection(cross_list))))\n",
    "    \n",
    "    added_df = pd.DataFrame(dict_att)\n",
    "    df = df.join(added_df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_csv(csv):\n",
    "    \"\"\"\n",
    "    This function loads the dataset from boliga annd preproccesses it.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv,index_col=\"Unnamed: 0\")\n",
    "    y = df[\"price\"]\n",
    "    \n",
    "    #Making room per sqm\n",
    "    df[\"rooms_per_sqm\"] = (df[\"rooms\"]/df[\"size\"]).replace(np.inf,0)\n",
    "    \n",
    "    \n",
    "    #Removing houses not placed in Denmark\n",
    "    df = df[(df[\"latitude\"]<60) & (df[\"latitude\"]>53) & (df[\"longitude\"]>7) & (df[\"longitude\"]<16)]\n",
    "    \n",
    "    #Generating dummy variables - Property type\n",
    "    housing_type={1:'villa',2:'raekkehuse', 3:'ejerlejlighed',4:'fritidshus', 5:'andel',6:'landejendom', \n",
    "              7:'helrsgrund',8:'fritidsgrund', 9:'villalejlighed',10:'andet_1',11:\"andet_2\",12:\"andet_3\"}\n",
    "    dummy_df = pd.get_dummies(df['propertyType'].replace(housing_type))   \n",
    "    df = df.join(dummy_df)\n",
    "    df.drop(\"propertyType\",axis=1,inplace=True)\n",
    "    #df.rename(columns = housing_type,inplace=True)\n",
    "    \n",
    "    #Dropping all None house\n",
    "    df = df[(df[\"helrsgrund\"] == 0) & (df[\"andet_3\"] == 0)]\n",
    "    \n",
    "    \n",
    "    #Generation dummies variables - Kommune\n",
    "    dummy_mun = pd.get_dummies(df[\"kommune_navn\"])\n",
    "    df = df.join(dummy_mun)\n",
    "    df.drop(\"kommune_navn\",inplace=True,axis=1)\n",
    "    \n",
    "    #Generation dummies variables - Floor\n",
    "    dummy_floor = pd.get_dummies(df[\"floor\"].map(str))\n",
    "    df = df.join(dummy_floor)\n",
    "    df.drop(\"floor\",inplace=True,axis=1)\n",
    "\n",
    "    \n",
    "    #Dropping all non essential columns\n",
    "    df.drop([\"downPayment\",\"estateUrl\",\"currentArchiveId\",\"forSaleNowId\",\n",
    "            \"foreclosureId\",\"cleanStreet\",\"estateId\",\"latitude\",\"longitude\",\"energyClass\",\n",
    "            \"price\",\"city\",\"isActive\",\"municipality\",\"zipCode\",\"street\",\"createdDate\",\n",
    "            \"squaremeterPrice\",\"region\",\"kommune_nr\",\"rooms\"],axis=1,inplace=True)\n",
    "    df.dropna(inplace=True,axis=0)\n",
    "    #Adding keyattr to df\n",
    "    #df = add_keyattr(df)\n",
    "    \n",
    "    X = df.values\n",
    "    y = y[df.index].values\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "def preprocess_text(string,nlp=nlp):\n",
    "    \"\"\"\n",
    "    This function takes a string and returns a list with all noun from string lemmatized\n",
    "    \"\"\"\n",
    "\n",
    "    #Removing everything but words\n",
    "    string = re.sub(r'[^\\w\\s]','',string)\n",
    "    \n",
    "    #Removing stopwords\n",
    "    stop_words_list = stopwords.words(\"danish\")\n",
    "    string = [i for i in nltk.word_tokenize(string.lower()) if i not in stop_words_list]\n",
    "    string = \" \".join(string)\n",
    "    \n",
    "    #Getting all nounce\n",
    "    string = nlp(string)\n",
    "    nouns = [word._.lemmas[0] for word in string if word.pos_ == \"NOUN\"]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "def words_count(list_of_strings):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings and returns a dict with counts of each word\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = list_of_strings\n",
    "    counts = dict(Counter(sentences))\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def keywords(body):\n",
    "    \"\"\"\n",
    "    This function takes a list of different strings and returns a dataframe with each word and word count\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = list()\n",
    "    \n",
    "    for bodys in tqdm.tqdm(body):\n",
    "        string = preprocess_text(bodys[0])\n",
    "        word_list.extend(string)\n",
    "        \n",
    "    dict_count = words_count(word_list)\n",
    "    df = pd.DataFrame({\"word\":list(dict_count.keys()),\n",
    "                        \"count\":list(dict_count.values())}).set_index(\"word\")\n",
    "    \n",
    "    #Getting the 300 largest counts returns the keywords used for key attr list\n",
    "    return df\n",
    "    \n",
    "def add_lonlat(df,df_station):\n",
    "    \"\"\"\n",
    "    This function takes two df and returns the distance between two locations of the dataframes location columns\n",
    "    \"\"\"\n",
    "    \n",
    "    min_dist = list()\n",
    "    for location in tqdm(df[\"location\"]):\n",
    "        distance = list()\n",
    "        for lon,lat in zip(df_station[\"lon\"],df_station[\"lat\"]):\n",
    "            distance.append(geodesic((lat,lon), location).km)\n",
    "    \n",
    "        min_dist.append(min(distance))\n",
    "        \n",
    "    df[\"dist_station\"] = min_dist\n",
    "    \n",
    "    return df\n",
    "\n",
    "def next_preprocess(X,y,random_state=None):\n",
    "    #Preparing transformer, for all but dummy variables\n",
    "    ct = ColumnTransformer([(\"poly\", PolynomialFeatures(degree=3),[0,11]),\n",
    "                    (\"scaler\",StandardScaler(),[0,11])],remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                        random_state=random_state,\n",
    "                                                        test_size=0.33)\n",
    "    \n",
    "    X_test = ct.fit_transform(X_test)\n",
    "    X_train = ct.transform(X_train)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_preprocess(X,y,random_state=None):\n",
    "    #Preparing transformer, for all but dummy variables\n",
    "    ct = ColumnTransformer([(\"poly\", PolynomialFeatures(degree=3),[0,11]),\n",
    "                    (\"scaler\",StandardScaler(),[0,11])],remainder=\"passthrough\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                        random_state=random_state,\n",
    "                                                        test_size=0.33)\n",
    "    \n",
    "    X_test = ct.fit_transform(X_test)\n",
    "    X_train = ct.transform(X_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores\",scores)\n",
    "    print(\"Mean:\",scores.mean())\n",
    "    print(\"Standard Deviation\",scores.std())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, df = preprocess_csv(\"house_data_final.csv\")\n",
    "X_train, X_test, y_train, y_test = next_preprocess(X,y)\n",
    "y=y/1e+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"poly\", PolynomialFeatures(degree=3),[0,11]),\n",
    "                    (\"scaler\",StandardScaler(),[0,11])],remainder=\"passthrough\")\n",
    "X_fitted = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [3.87106866 3.95474695 1.32774285 1.84578014 1.52570378 1.7714184\n",
      " 1.81491189 1.64078817 1.31925745 1.56321443]\n",
      "Mean: 2.063463271531557\n",
      "Standard Deviation 0.9408538897664241\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_scores = cross_val_score(lin_reg,X_fitted,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [2.37571006 3.43716916 1.15790938 1.4370493  1.33311078 1.3277558\n",
      " 1.00131393 1.41242737 0.98313091 1.076652  ]\n",
      "Mean: 1.554222868987765\n",
      "Standard Deviation 0.7328938585084165\n"
     ]
    }
   ],
   "source": [
    "ran_reg = RandomForestRegressor(max_features=6,n_estimators = 54) #Getting best estimators from gridsearch\n",
    "ran_scores = cross_val_score(ran_reg,X,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-ran_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [2.96121526 3.84172847 1.91591413 1.85621071 1.73629982 1.58112328\n",
      " 1.28816535 1.81378853 1.52725955 1.44576994]\n",
      "Mean: 1.9967475040121507\n",
      "Standard Deviation 0.7523677163932851\n"
     ]
    }
   ],
   "source": [
    "des_reg = DecisionTreeRegressor()\n",
    "tree_scores = cross_val_score(des_reg,X,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=2.61898e-25): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.46438e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.72203e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.4632e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.48142e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.46738e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=9.31619e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.46706e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [3.86169066 3.9538282  1.31777743 1.84402866 1.52502447 1.76807829\n",
      " 1.80359446 1.63696014 1.31769497 1.53197813]\n",
      "Mean: 2.0560655398929244\n",
      "Standard Deviation 0.9421255298794935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.46384e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.842e-26): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = Ridge(max_iter=2000)\n",
    "ridge_scores = cross_val_score(ridge_reg,X_fitted,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "ridge_rmse_scores = np.sqrt(-ridge_scores)\n",
    "display_scores(ridge_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81044.81618795171, tolerance: 21.90654650860169\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6975.457144389147, tolerance: 17.705942172738705\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108289.26325321084, tolerance: 27.44555149539335\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105332.52706245778, tolerance: 26.89234018791579\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106070.89056143213, tolerance: 27.30791185482623\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105069.1221603552, tolerance: 26.7769415553908\n",
      "  positive)\n",
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106205.07141545374, tolerance: 27.47831196916766\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [4.05601726 4.31400801 1.23891026 1.65755428 1.65328653 1.55707476\n",
      " 1.61539636 1.68697996 1.36823058 1.66833724]\n",
      "Mean: 2.0815795247371773\n",
      "Standard Deviation 1.0622260482281622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederikfindsen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106103.81132891218, tolerance: 27.41008303680032\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(max_iter=2000)\n",
    "lasso_scores = cross_val_score(lasso_reg,X_fitted,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "lasso_rmse_scores = np.sqrt(-lasso_scores)\n",
    "display_scores(lasso_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6703916855173625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = next_preprocess(X,y)\n",
    "ran_test = RandomForestRegressor(max_features=6,n_estimators=55)\n",
    "ran_test.fit(X_train,y_train)\n",
    "r2_score(y_test,ran_test.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting best estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=9, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features=6,\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=54, n_jobs=None,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'n_estimators': [50, 51, 52, 53, 54, 55, 56, 57, 58,\n",
       "                                          59]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"n_estimators\":list(range(50,60))}\n",
    "ran_search = GridSearchCV(ran_reg,param_grid,cv=9,scoring=\"neg_mean_squared_error\")\n",
    "ran_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 58}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_range = np.arange(1, 100, 2)\n",
    "#Making validation curve\n",
    "train_scores, test_scores = validation_curve(RandomForestRegressor(), \n",
    "                                             X, \n",
    "                                             y, \n",
    "                                             param_name=\"n_estimators\", \n",
    "                                             param_range=param_range,\n",
    "                                             cv=3, \n",
    "                                             scoring=\"neg_mean_squared_error\")\n",
    "#Calculating Mean\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "ax.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "# Plot accurancy bands for training and test sets\n",
    "ax.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
    "ax.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "# Create plot\n",
    "ax.title(\"Validation Curve With Random Forest\")\n",
    "ax.xlabel(\"Number Of Trees\")\n",
    "ax.ylabel(\"Neg Mean Squared Error\")\n",
    "ax.tight_layout()\n",
    "ax.legend(loc=\"best\")\n",
    "ax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
